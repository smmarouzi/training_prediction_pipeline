{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.39.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-cloud-aiplatform)\n",
      "  Using cached proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform)\n",
      "  Using cached protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: packaging>=14.3 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_storage-2.14.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_bigquery-3.15.0-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_resource_manager-1.11.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform)\n",
      "  Using cached shapely-2.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached google_auth-2.26.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached grpcio-1.60.0-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached grpcio_status-1.60.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
      "  Using cached grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached google_crc32c-1.5.0-cp311-cp311-macosx_10_9_universal2.whl (32 kB)\n",
      "Requirement already satisfied: numpy>=1.14 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached google_cloud_aiplatform-1.39.0-py2.py3-none-any.whl (3.4 MB)\n",
      "Using cached google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
      "Using cached google_cloud_bigquery-3.15.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached google_cloud_resource_manager-1.11.0-py2.py3-none-any.whl (320 kB)\n",
      "Using cached google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
      "Using cached proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "Using cached protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached shapely-2.0.2-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Using cached google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
      "Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Using cached grpcio-1.60.0-cp311-cp311-macosx_10_10_universal2.whl (9.7 MB)\n",
      "Using cached grpcio_status-1.60.0-py3-none-any.whl (14 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: urllib3, shapely, pyasn1, protobuf, idna, grpcio, google-crc32c, charset-normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, grpc-google-iam-v1, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Successfully installed cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 google-api-core-2.15.0 google-auth-2.26.2 google-cloud-aiplatform-1.39.0 google-cloud-bigquery-3.15.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.11.0 google-cloud-storage-2.14.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.62.0 grpc-google-iam-v1-0.13.0 grpcio-1.60.0 grpcio-status-1.60.0 idna-3.6 proto-plus-1.23.0 protobuf-4.25.2 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 rsa-4.9 shapely-2.0.2 urllib3-2.1.0\n",
      "Collecting kfp\n",
      "  Using cached kfp-2.6.0-py3-none-any.whl\n",
      "Collecting click<9,>=8.0.0 (from kfp)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3 (from kfp)\n",
      "  Using cached docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp) (2.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp) (2.26.2)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp) (2.14.0)\n",
      "Collecting kfp-pipeline-spec==0.3.0 (from kfp)\n",
      "  Using cached kfp_pipeline_spec-0.3.0-py3-none-any.whl.metadata (329 bytes)\n",
      "Collecting kfp-server-api<2.1.0,>=2.0.0 (from kfp)\n",
      "  Using cached kfp_server_api-2.0.5-py3-none-any.whl\n",
      "Collecting kubernetes<27,>=8.0.0 (from kfp)\n",
      "  Using cached kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp) (4.25.2)\n",
      "Collecting PyYAML<7,>=5.3 (from kfp)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests-toolbelt<1,>=0.8.0 (from kfp)\n",
      "  Using cached requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting tabulate<1,>=0.8.6 (from kfp)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting urllib3<2.0.0 (from kfp)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.62.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from kubernetes<27,>=8.0.0->kfp) (68.2.2)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes<27,>=8.0.0->kfp)\n",
      "  Using cached websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes<27,>=8.0.0->kfp)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/smarouzi/miniconda3/envs/SkipTheDishes/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.6)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Using cached websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: websocket-client, urllib3, tabulate, PyYAML, oauthlib, kfp-pipeline-spec, docstring-parser, click, kfp-server-api, requests-toolbelt, requests-oauthlib, kubernetes, kfp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed PyYAML-6.0.1 click-8.1.7 docstring-parser-0.15 kfp-2.6.0 kfp-pipeline-spec-0.3.0 kfp-server-api-2.0.5 kubernetes-26.1.0 oauthlib-3.2.2 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 tabulate-0.9.0 urllib3-1.26.18 websocket-client-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform\n",
    "!pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_90982/2783593710.py:3: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl\n"
     ]
    }
   ],
   "source": [
    "# IMPORT THE REQUIRED LIBRARIES\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Output,\n",
    "                        Model,\n",
    "                        Metrics,\n",
    "                        Markdown,\n",
    "                        HTML,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform as vertex_\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"skip-the-dishes-410816\"\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"skipthedishes\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root_houseprice/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom base image created using docker\n",
    "\n",
    "IMAGE_NAME = \"training\"\n",
    "BASE_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/busynessestimation/{IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_90982/3544479945.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"get_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_std_data(\n",
    "    filepath: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    df_train = pd.read_csv(filepath + '/train.csv')\n",
    "   \n",
    "    df_train.to_csv(dataset_train.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"preprocessing.yaml\"\n",
    ")\n",
    "\n",
    "def preprocess_std_data(\n",
    "    train_df: Input[Dataset],\n",
    "    dataset_train_preprocessed: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from src.prepare_data.preprocessing import data_preprocessing_pipeline\n",
    "   \n",
    "    train_df = pd.read_csv(train_df.path)\n",
    "    \n",
    "    train_df_preprocessed = data_preprocessing_pipeline(train_df)\n",
    "    \n",
    "    train_df_preprocessed.to_csv(dataset_train_preprocessed.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_90982/829962897.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"train_test_split.yaml\",\n",
    ")\n",
    "def train_test_split(dataset_in: Input[Dataset],\n",
    "                     dataset_train: Output[Dataset],\n",
    "                     dataset_test: Output[Dataset],\n",
    "                     test_size: float = 0.33):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(dataset_in.path)\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "    df_train.to_csv(dataset_train.path, index=False)\n",
    "    df_test.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_busyness(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    best_params: Output[Markdown],\n",
    "    shap_summary_plot: Output[HTML],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import shap\n",
    "    from src.train.model import BusynessEstimation\n",
    "    from src.train.config import Data\n",
    "    from src.utils.utils import get_image_data\n",
    "    \n",
    "    \n",
    "    # Read train and test data\n",
    "    train_data = pd.read_csv(dataset_train.path)\n",
    "    test_data = pd.read_csv(dataset_test.path)\n",
    "    \n",
    "    # Instantiate the model class\n",
    "    busyness_model = BusynessEstimation(\n",
    "                                        test_data.copy()\n",
    "                                        )\n",
    "                                        \n",
    "    # Create X_train and y_train\n",
    "    X_train = train_data.drop(Data.target, axis=1)\n",
    "    y_train = train_data[Data.target]\n",
    "\n",
    "    # Fit the model (training pipeline consists of feature engineering, feature selection and training an xgboost model)\n",
    "    busyness_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the best hyperparameters as an artifact\n",
    "    with open(best_params.path, \"w\") as f:\n",
    "        f.write(str(busyness_model.best_params))\n",
    "    \"\"\"    \n",
    "    shap.summary_plot(busyness_model.shap_values, busyness_model.X_test_transformed, max_display=20) # plot shap summary plot\n",
    "    shap_plot_dataurl = get_image_data() # get image data to render the image in the html file\n",
    "    html_content = f'<html><head></head><body><h1>Shap Summary Plot</h1>\\n<img src={shap_plot_dataurl} width=\"97%\"></body></html>' \n",
    "    # Save shap summary plot as an html artifact\n",
    "    with open(shap_summary_plot.path, \"w\") as f: \n",
    "        f.write(html_content)\n",
    "    \"\"\"\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\" \n",
    "    # Save the model as an artifact\n",
    "    with open(model.path, 'wb') as f: \n",
    "        pickle.dump({\n",
    "            \"pipeline\": busyness_model.model_pipeline,\n",
    "            \"target\": busyness_model.target,\n",
    "            \"scores_dict\": busyness_model.scores}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"model_evaluation.yaml\"\n",
    ")\n",
    "def evaluate_busyness(\n",
    "    busyness_model: Input[Model],\n",
    "    metrics_baseline: Output[Metrics],\n",
    "    metrics_train: Output[Metrics],\n",
    "    metrics_test: Output[Metrics]):\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    file_name = busyness_model.path\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model_data = pickle.load(file)\n",
    "    \n",
    "    scores = model_data[\"scores_dict\"] \n",
    "\n",
    "    def log_metrics(scores, metric):\n",
    "        for metric_name, val in scores.items():\n",
    "            metric.log_metric(metric_name, float(val))\n",
    "            \n",
    "    log_metrics(scores[\"baseline_scores\"], metrics_baseline)        \n",
    "    log_metrics(scores[\"train_scores\"], metrics_train)\n",
    "    log_metrics(scores[\"test_scores\"], metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    install_kfp_package=False,\n",
    "    output_component_file=\"model_deployment.yaml\",\n",
    ")\n",
    "def deploy_busyness(\n",
    "        serving_container_image_uri: str,\n",
    "        display_name: str,\n",
    "        model_endpoint: str,\n",
    "        gcp_project: str,\n",
    "        gcp_region: str,\n",
    "        model: Input[Model],\n",
    "        vertex_model: Output[Model],\n",
    "        vertex_endpoint: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Checks existing Vertex AI Enpoint or creates Endpoint if it is not exist.\n",
    "    def create_endpoint ():\n",
    "        endpoints = vertex_ai.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(model_endpoint),\n",
    "        order_by='create_time desc',\n",
    "        project=gcp_project,\n",
    "        location=gcp_region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0] # most recently created\n",
    "        else:\n",
    "            endpoint = vertex_ai.Endpoint.create(\n",
    "                display_name=model_endpoint,\n",
    "                project=gcp_project,\n",
    "                location=gcp_region\n",
    "        )\n",
    "        return endpoint\n",
    "\n",
    "    endpoint = create_endpoint()\n",
    "    \n",
    "    # Uploads trained model to Vertex AI Model Registry or creates new model version into existing uploaded one.\n",
    "    def upload_model ():\n",
    "        listed_model = vertex_ai.Model.list(\n",
    "        filter='display_name=\"{}\"'.format(display_name),\n",
    "        project=gcp_project,\n",
    "        location=gcp_region,\n",
    "        )\n",
    "        if len(listed_model) > 0:\n",
    "            model_version = listed_model[0] # most recently created\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    parent_model=model_version.resource_name,\n",
    "                    artifact_uri=str(Path(model.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        else:\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    artifact_uri=str(Path(model.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        return model_upload\n",
    "    \n",
    "    uploaded_model = upload_model()\n",
    "    \n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = uploaded_model.resource_name\n",
    "\n",
    "    # Deploys trained model to Vertex AI Endpoint\n",
    "    model_deploy = uploaded_model.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-busyness-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-regions-busyness\"   \n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    data_filepath: str = f\"{BUCKET_NAME}/data\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,    \n",
    "    serving_container_image_uri: str = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/houseprice/serving_image:latest\" # custom serving container image     \n",
    "):\n",
    "\n",
    "    data_op = get_std_data(data_filepath)\n",
    "    data_preprocess_op = preprocess_std_data(data_op.outputs[\"dataset_train\"])\n",
    "    train_test_split_op = train_test_split(data_preprocess_op.outputs[\"dataset_train_preprocessed\"])\n",
    "    train_model_op = train_busyness(train_test_split_op.outputs[\"dataset_train\"], train_test_split_op.outputs[\"dataset_test\"])\n",
    "    model_evaluation_op = evaluate_busyness(train_model_op.outputs[\"model\"])\n",
    "           \n",
    "    deploy_model_op = deploy_busyness(\n",
    "        model = train_model_op.outputs['model'],\n",
    "        gcp_project = project,\n",
    "        gcp_region = region, \n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        display_name = \"region_busyness\",\n",
    "        model_endpoint = \"region_busyness_endpoint\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Run the pipelie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE (to create the job spec file)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_region_busyness.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A RUN USING THE JOB SPEC FILE GENERATED \n",
    "\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"regionbusyness-pipeline\",\n",
    "    template_path=\"ml_region_busyness.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE PIPELINE\n",
    "\n",
    "start_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions Using Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "endpoint_name = <ENDPOINT_URI>\n",
    "endpoint = vertex_ai.Endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = test_df.to_json(orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=request.splitlines())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SkipTheDishes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
