{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "id",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORT THE REQUIRED LIBRARIES\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Output,\n",
    "                        Model,\n",
    "                        Metrics,\n",
    "                        Markdown,\n",
    "                        HTML,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp import compiler\n",
    "from google.cloud import aiplatform as vertex_\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3bc836-0ebe-461b-9143-83f362ef5a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"id\"\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID+\"-rb\"\n",
    "source_data_blob = \"data/data.csv\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root_rb/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c35da1d-90f5-4676-98df-635ab4418784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://id-rb/pipeline_root_rb/\n"
     ]
    }
   ],
   "source": [
    "print(PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5a4342-b3b1-4280-997e-cc819fd5a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/id-410816/rb/training:latest\n"
     ]
    }
   ],
   "source": [
    "# Custom base image created using docker\n",
    "\n",
    "IMAGE_NAME = \"training\"\n",
    "BASE_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/rb/{IMAGE_NAME}:latest\"\n",
    "print(BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfed53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"GOOGLE_APPLICATION_CREDENTIALS\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = input(\"Enter path to Google Cloud service account key JSON: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cfc7fca-561f-4ac3-8493-15bfaf28a7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def create_bucket_class_location(bucket_name):\n",
    "    \"\"\"\n",
    "    Create a new bucket in the US region with the coldline storage\n",
    "    class\n",
    "    \"\"\"\n",
    "    # bucket_name = \"your-new-bucket-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.storage_class = \"COLDLINE\"\n",
    "    new_bucket = storage_client.create_bucket(bucket, location=\"us-central1\")\n",
    "\n",
    "    print(\n",
    "        \"Created bucket {} in {} with storage class {}\".format(\n",
    "            new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
    "        )\n",
    "    )\n",
    "    return new_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "650b8aff-5012-4ac9-b5de-b8c0179a0bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea94bb86-8bfe-4eb2-a5ae-064d0a66a0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def write_read(bucket_name, blob_name):\n",
    "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your new GCS object\n",
    "    # blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    content = blob.download_as_bytes()\n",
    "\n",
    "    df = pd.read_csv(io.BytesIO(content))\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d247743-d89c-456d-a96e-198fb3219f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = write_read(BUCKET_NAME, \"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c966034-e9c8-4f57-b89a-b6ebfcb39f38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courier_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>courier_location_timestamp</th>\n",
       "      <th>courier_lat</th>\n",
       "      <th>courier_lon</th>\n",
       "      <th>order_created_timestamp</th>\n",
       "      <th>restaurant_lat</th>\n",
       "      <th>restaurant_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a98737cbhoho5012hoho4b5bhoho867fhoho8475c658546d</td>\n",
       "      <td>281289453</td>\n",
       "      <td>2021-04-02T04:30:42.328Z</td>\n",
       "      <td>50.484520</td>\n",
       "      <td>-104.618876</td>\n",
       "      <td>2021-04-02T04:20:42Z</td>\n",
       "      <td>50.483696</td>\n",
       "      <td>-104.614350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39a26fa0hohof428hoho47a4hohoa320hoho12e3d831c23a</td>\n",
       "      <td>280949566</td>\n",
       "      <td>2021-04-01T06:14:47.386Z</td>\n",
       "      <td>50.442573</td>\n",
       "      <td>-104.550463</td>\n",
       "      <td>2021-04-01T06:05:18Z</td>\n",
       "      <td>50.442422</td>\n",
       "      <td>-104.550487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3813235ehoho7a42hoho4601hohob7eahoho799e8af5b535</td>\n",
       "      <td>281328578</td>\n",
       "      <td>2021-04-02T05:48:57.224Z</td>\n",
       "      <td>50.495920</td>\n",
       "      <td>-104.635605</td>\n",
       "      <td>2021-04-02T05:13:26Z</td>\n",
       "      <td>50.496595</td>\n",
       "      <td>-104.635606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9f033953hohocd53hoho488ahohoaf51hohoc57943e499ed</td>\n",
       "      <td>281317998</td>\n",
       "      <td>2021-04-02T05:12:17.252Z</td>\n",
       "      <td>50.449445</td>\n",
       "      <td>-104.611521</td>\n",
       "      <td>2021-04-02T04:59:57Z</td>\n",
       "      <td>50.449504</td>\n",
       "      <td>-104.611074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56f65bc8hohoba54hoho47dfhohoa09chohof7464b5d9848</td>\n",
       "      <td>281314132</td>\n",
       "      <td>2021-04-02T05:15:38.266Z</td>\n",
       "      <td>50.495254</td>\n",
       "      <td>-104.666383</td>\n",
       "      <td>2021-04-02T04:54:53Z</td>\n",
       "      <td>50.495160</td>\n",
       "      <td>-104.665733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         courier_id  order_number  \\\n",
       "0  a98737cbhoho5012hoho4b5bhoho867fhoho8475c658546d     281289453   \n",
       "1  39a26fa0hohof428hoho47a4hohoa320hoho12e3d831c23a     280949566   \n",
       "2  3813235ehoho7a42hoho4601hohob7eahoho799e8af5b535     281328578   \n",
       "3  9f033953hohocd53hoho488ahohoaf51hohoc57943e499ed     281317998   \n",
       "4  56f65bc8hohoba54hoho47dfhohoa09chohof7464b5d9848     281314132   \n",
       "\n",
       "  courier_location_timestamp  courier_lat  courier_lon  \\\n",
       "0   2021-04-02T04:30:42.328Z    50.484520  -104.618876   \n",
       "1   2021-04-01T06:14:47.386Z    50.442573  -104.550463   \n",
       "2   2021-04-02T05:48:57.224Z    50.495920  -104.635605   \n",
       "3   2021-04-02T05:12:17.252Z    50.449445  -104.611521   \n",
       "4   2021-04-02T05:15:38.266Z    50.495254  -104.666383   \n",
       "\n",
       "  order_created_timestamp  restaurant_lat  restaurant_lon  \n",
       "0    2021-04-02T04:20:42Z       50.483696     -104.614350  \n",
       "1    2021-04-01T06:05:18Z       50.442422     -104.550487  \n",
       "2    2021-04-02T05:13:26Z       50.496595     -104.635606  \n",
       "3    2021-04-02T04:59:57Z       50.449504     -104.611074  \n",
       "4    2021-04-02T04:54:53Z       50.495160     -104.665733  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66472a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fsspec\n",
    "#!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12723133-926e-41b5-9ad1-acd6c8fea58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"gs://{BUCKET_NAME}/{source_data_blob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e24662-1f90-4edf-b82a-da5b3b9f381c",
   "metadata": {},
   "source": [
    "## Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ecaced7f-0c4d-4b51-bccc-92d89c8f1bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_23685/1386007704.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"get_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_std_data(\n",
    "    gcs_bucket: str,\n",
    "    gcs_path: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "):\n",
    "    # Set the service account key file for authentication\n",
    "    \n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    # print(f\"gs://{gcs_bucket}/{gcs_path}\")\n",
    "    df = pd.read_csv(f\"gs://{gcs_bucket}/{gcs_path}\")\n",
    "\n",
    "    df.to_csv(dataset_train.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01855d35-3b53-46c2-8db4-fd2f26c47a05",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c58792b-d7e2-4d34-83f9-31e0d7948659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_23685/1988278445.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"preprocessing.yaml\"\n",
    ")\n",
    "\n",
    "def preprocess_std_data(\n",
    "    train_df: Input[Dataset],\n",
    "    dataset_train_preprocessed: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from src.prepare_data.preprocessing import data_preprocessing_pipeline\n",
    "   \n",
    "    train_df = pd.read_csv(train_df.path)\n",
    "    \n",
    "    train_df_preprocessed = data_preprocessing_pipeline(train_df)\n",
    "    \n",
    "    train_df_preprocessed.to_csv(dataset_train_preprocessed.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d4b78-efe3-45a9-b0f6-1749fd5c7b1a",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65aa16be-21e0-494c-9ac0-60cc4609aa17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_23685/593052275.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"train_test_split.yaml\",\n",
    ")\n",
    "def train_test_split(dataset_in: Input[Dataset],\n",
    "                     dataset_train: Output[Dataset],\n",
    "                     dataset_test: Output[Dataset],\n",
    "                     test_size: float = 0.33):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(dataset_in.path)\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "    df_train.to_csv(dataset_train.path, index=False)\n",
    "    df_test.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78329a2c-b1f0-455e-b61c-9fd9e341c316",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24e8bacf-700c-4139-b50f-c98a750c8ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_23685/1657412487.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"model_training.yaml\"\n",
    ")\n",
    "def train_busyness(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from src.train.model import BusynessEstimation\n",
    "    from src.train.config import Data\n",
    "    from src.utils.utils import get_image_data\n",
    "    \n",
    "    \n",
    "    # Read train and test data\n",
    "    train_data = pd.read_csv(dataset_train.path)\n",
    "    test_data = pd.read_csv(dataset_test.path)\n",
    "    \n",
    "    # Instantiate the model class\n",
    "    busyness_model = BusynessEstimation(\n",
    "                                        test_data.copy()\n",
    "                                        )\n",
    "                                        \n",
    "    # Create X_train and y_train\n",
    "    X_train = train_data.drop(Data.target, axis=1)\n",
    "    y_train = train_data[Data.target]\n",
    "\n",
    "    # Fit the model (training pipeline consists of feature engineering, feature selection and training an xgboost model)\n",
    "    busyness_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the best hyperparameters as an artifact\n",
    "    with open(best_params.path, \"w\") as f:\n",
    "        f.write(str(busyness_model.best_params))\n",
    "    \"\"\"    \n",
    "    shap.summary_plot(busyness_model.shap_values, busyness_model.X_test_transformed, max_display=20) # plot shap summary plot\n",
    "    shap_plot_dataurl = get_image_data() # get image data to render the image in the html file\n",
    "    html_content = f'<html><head></head><body><h1>Shap Summary Plot</h1>\\n<img src={shap_plot_dataurl} width=\"97%\"></body></html>' \n",
    "    # Save shap summary plot as an html artifact\n",
    "    with open(shap_summary_plot.path, \"w\") as f: \n",
    "        f.write(html_content)\n",
    "    \"\"\"\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\" \n",
    "    # Save the model as an artifact\n",
    "    with open(model.path, 'wb') as f: \n",
    "        pickle.dump({\n",
    "            \"pipeline\": busyness_model.model_pipeline,\n",
    "            \"target\": busyness_model.target,\n",
    "            \"scores_dict\": busyness_model.scores}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95f191-eca3-4fb0-90f1-6525c525b679",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5030b66-2e40-4215-b41f-3ffe598f2df5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_23685/2105362814.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    output_component_file=\"model_evaluation.yaml\"\n",
    ")\n",
    "def evaluate_busyness(\n",
    "    busyness_model: Input[Model],\n",
    "    metrics_baseline: Output[Metrics],\n",
    "    metrics_train: Output[Metrics],\n",
    "    metrics_test: Output[Metrics]):\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    file_name = busyness_model.path\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model_data = pickle.load(file)\n",
    "    \n",
    "    scores = model_data[\"scores_dict\"] \n",
    "\n",
    "    def log_metrics(scores, metric):\n",
    "        for metric_name, val in scores.items():\n",
    "            metric.log_metric(metric_name, float(val))\n",
    "            \n",
    "    log_metrics(scores[\"baseline_scores\"], metrics_baseline)        \n",
    "    log_metrics(scores[\"train_scores\"], metrics_train)\n",
    "    log_metrics(scores[\"test_scores\"], metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461f86f-bbd1-47ed-a6fa-bff92213239a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7a26372-9e07-4edc-9fc0-2a7188aab032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/sbp8v1ys5wz6pm1xg3jkgnpm0000gn/T/ipykernel_23685/1137965636.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    install_kfp_package=False,\n",
    "    output_component_file=\"model_deployment.yaml\",\n",
    ")\n",
    "def deploy_busyness(\n",
    "        serving_container_image_uri: str,\n",
    "        display_name: str,\n",
    "        model_endpoint: str,\n",
    "        gcp_project: str,\n",
    "        gcp_region: str,\n",
    "        model: Input[Model],\n",
    "        vertex_model: Output[Model],\n",
    "        vertex_endpoint: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Checks existing Vertex AI Enpoint or creates Endpoint if it is not exist.\n",
    "    def create_endpoint ():\n",
    "        endpoints = vertex_ai.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(model_endpoint),\n",
    "        order_by='create_time desc',\n",
    "        project=gcp_project,\n",
    "        location=gcp_region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0] # most recently created\n",
    "        else:\n",
    "            endpoint = vertex_ai.Endpoint.create(\n",
    "                display_name=model_endpoint,\n",
    "                project=gcp_project,\n",
    "                location=gcp_region\n",
    "        )\n",
    "        return endpoint\n",
    "\n",
    "    endpoint = create_endpoint()\n",
    "    \n",
    "    # Uploads trained model to Vertex AI Model Registry or creates new model version into existing uploaded one.\n",
    "    def upload_model ():\n",
    "        listed_model = vertex_ai.Model.list(\n",
    "        filter='display_name=\"{}\"'.format(display_name),\n",
    "        project=gcp_project,\n",
    "        location=gcp_region,\n",
    "        )\n",
    "        if len(listed_model) > 0:\n",
    "            model_version = listed_model[0] # most recently created\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    parent_model=model_version.resource_name,\n",
    "                    artifact_uri=str(Path(model.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        else:\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    artifact_uri=str(Path(model.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        return model_upload\n",
    "    \n",
    "    uploaded_model = upload_model()\n",
    "    \n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = uploaded_model.resource_name\n",
    "\n",
    "    # Deploys trained model to Vertex AI Endpoint\n",
    "    model_deploy = uploaded_model.deploy(\n",
    "        machine_type='n1-standard-1',\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac68c79-48b6-4e0d-a992-a10db5763e6c",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3559983-00bd-4b73-9085-af190b67950b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipelinebusyness-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9365f534-c885-431e-9ac5-3f04c52bbd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serving_container_image_uri = \"gcr.io/your-project/serving-image:latest\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-regionsbusyness\"   \n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    data_filepath: str = \"data/data.csv\",\n",
    "    data_bucket : str = BUCKET_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION    \n",
    "):\n",
    "\n",
    "    data_op = get_std_data(gcs_bucket = data_bucket, gcs_path = data_filepath )\n",
    "    data_preprocess_op = preprocess_std_data(train_df = data_op.outputs[\"dataset_train\"])\n",
    "    train_test_split_op = train_test_split(dataset_in = data_preprocess_op.outputs[\"dataset_train_preprocessed\"])\n",
    "    train_model_op = train_busyness(dataset_train = train_test_split_op.outputs[\"dataset_train\"], dataset_test = train_test_split_op.outputs[\"dataset_test\"])\n",
    "    model_evaluation_op = evaluate_busyness(busyness_model = train_model_op.outputs[\"model\"])\n",
    "    \n",
    "    deploy_model_op = deploy_busyness(\n",
    "        model = train_model_op.outputs['model'],\n",
    "        gcp_project = project,\n",
    "        gcp_region = region, \n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        display_name = \"rb\",\n",
    "        model_endpoint = \"rb_endpoint\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c0754",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d7caaf3-c2cd-4a00-a29d-583f0e8832d0",
   "metadata": {},
   "source": [
    "## Compile and Run the pipelie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "648d586c-a063-4413-a898-87fc4cdd5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE (to create the job spec file)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_rb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9e1f920-c0a0-40a7-9827-58d15a70a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A RUN USING THE JOB SPEC FILE GENERATED \n",
    "\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"rb-pipeline\",\n",
    "    template_path=\"ml_rb.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f8dd691-60b6-464e-ada5-095d3d37f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/311923924433/locations/us-central1/pipelineJobs/pipeline-rb-20240117104617\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/311923924433/locations/us-central1/pipelineJobs/pipeline-rb-20240117104617')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-rb-20240117104617?project=311923924433\n",
      "PipelineJob projects/311923924433/locations/us-central1/pipelineJobs/pipeline-rb-20240117104617 current state:\n",
      "3\n",
      "PipelineJob projects/311923924433/locations/us-central1/pipelineJobs/pipeline-rb-20240117104617 current state:\n",
      "3\n",
      "PipelineJob projects/311923924433/locations/us-central1/pipelineJobs/pipeline-rb-20240117104617 current state:\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [get-std-data].; Job (project_id = id-410816, job_id = 4861649266763890688) is failed due to the above error.; Failed to handle the job: {project_number = 311923924433, job_id = 4861649266763890688}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RUN THE PIPELINE\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstart_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/STD/lib/python3.11/site-packages/google/cloud/aiplatform/pipeline_jobs.py:323\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/STD/lib/python3.11/site-packages/google/cloud/aiplatform/base.py:817\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    816\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    820\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/STD/lib/python3.11/site-packages/google/cloud/aiplatform/pipeline_jobs.py:366\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    360\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    361\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m    362\u001b[0m     reserved_ip_ranges\u001b[38;5;241m=\u001b[39mreserved_ip_ranges,\n\u001b[1;32m    363\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/STD/lib/python3.11/site-packages/google/cloud/aiplatform/pipeline_jobs.py:615\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [get-std-data].; Job (project_id = id-410816, job_id = 4861649266763890688) is failed due to the above error.; Failed to handle the job: {project_number = 311923924433, job_id = 4861649266763890688}\"\n"
     ]
    }
   ],
   "source": [
    "# RUN THE PIPELINE\n",
    "\n",
    "start_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39214801-6a5b-4c71-b637-e86a76becc21",
   "metadata": {},
   "source": [
    "## Make Predictions Using Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387d01f-9e27-469b-ac16-2b0d06799b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "endpoint_name = <ENDPOINT_URI>\n",
    "endpoint = vertex_ai.Endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9c51e-8c4d-40d4-8e4b-292a0704d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4019b4c-8521-4d2a-889d-3e216ae5bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = test_df.to_json(orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54de62-aaa0-41bd-a149-4cbb53dae9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=request.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773799b-b90e-4b2d-9a22-8be2b675f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}