{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "id",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORT THE REQUIRED LIBRARIES\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Output,\n",
    "                        Model,\n",
    "                        Metrics,\n",
    "                        Markdown,\n",
    "                        HTML,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp import compiler\n",
    "from google.cloud import aiplatform as vertex_\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bc836-0ebe-461b-9143-83f362ef5a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"id\"\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID+\"-rb\"\n",
    "source_data_blob = \"data/data.csv\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root_rb/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35da1d-90f5-4676-98df-635ab4418784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a4342-b3b1-4280-997e-cc819fd5a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom base image created using docker\n",
    "\n",
    "IMAGE_NAME = \"training\"\n",
    "BASE_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/rb/{IMAGE_NAME}:latest\"\n",
    "print(BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"GOOGLE_APPLICATION_CREDENTIALS\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = input(\"Enter path to Google Cloud service account key JSON: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc7fca-561f-4ac3-8493-15bfaf28a7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def create_bucket_class_location(bucket_name):\n",
    "    \"\"\"\n",
    "    Create a new bucket in the US region with the coldline storage\n",
    "    class\n",
    "    \"\"\"\n",
    "    # bucket_name = \"your-new-bucket-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.storage_class = \"COLDLINE\"\n",
    "    new_bucket = storage_client.create_bucket(bucket, location=\"us-central1\")\n",
    "\n",
    "    print(\n",
    "        \"Created bucket {} in {} with storage class {}\".format(\n",
    "            new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
    "        )\n",
    "    )\n",
    "    return new_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b8aff-5012-4ac9-b5de-b8c0179a0bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94bb86-8bfe-4eb2-a5ae-064d0a66a0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def write_read(bucket_name, blob_name):\n",
    "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your new GCS object\n",
    "    # blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    content = blob.download_as_bytes()\n",
    "\n",
    "    df = pd.read_csv(io.BytesIO(content))\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d247743-d89c-456d-a96e-198fb3219f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = write_read(BUCKET_NAME, \"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c966034-e9c8-4f57-b89a-b6ebfcb39f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66472a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fsspec\n",
    "#!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12723133-926e-41b5-9ad1-acd6c8fea58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"gs://{BUCKET_NAME}/{source_data_blob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e24662-1f90-4edf-b82a-da5b3b9f381c",
   "metadata": {},
   "source": [
    "## Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaced7f-0c4d-4b51-bccc-92d89c8f1bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "\n",
    "def get_std_data(\n",
    "    gcs_bucket: str,\n",
    "    gcs_path: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "):\n",
    "    # Set the service account key file for authentication\n",
    "    \n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    # print(f\"gs://{gcs_bucket}/{gcs_path}\")\n",
    "    df = pd.read_csv(f\"gs://{gcs_bucket}/{gcs_path}\")\n",
    "\n",
    "    df.to_csv(dataset_train.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01855d35-3b53-46c2-8db4-fd2f26c47a05",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58792b-d7e2-4d34-83f9-31e0d7948659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "\n",
    "def preprocess_std_data(\n",
    "    train_df: Input[Dataset],\n",
    "    dataset_train_preprocessed: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from src.prepare_data.preprocessing import data_preprocessing_pipeline\n",
    "   \n",
    "    train_df = pd.read_csv(train_df.path)\n",
    "    \n",
    "    train_df_preprocessed = data_preprocessing_pipeline(train_df)\n",
    "    \n",
    "    train_df_preprocessed.to_csv(dataset_train_preprocessed.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d4b78-efe3-45a9-b0f6-1749fd5c7b1a",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa16be-21e0-494c-9ac0-60cc4609aa17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def train_test_split(dataset_in: Input[Dataset],\n",
    "                     dataset_train: Output[Dataset],\n",
    "                     dataset_test: Output[Dataset],\n",
    "                     test_size: float = 0.33):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(dataset_in.path)\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "    df_train.to_csv(dataset_train.path, index=False)\n",
    "    df_test.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78329a2c-b1f0-455e-b61c-9fd9e341c316",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8bacf-700c-4139-b50f-c98a750c8ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def train_busyness(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from src.train.model import BusynessEstimation\n",
    "    from src.train.config import Data\n",
    "    from src.utils.utils import get_image_data\n",
    "    \n",
    "    \n",
    "    # Read train and test data\n",
    "    train_data = pd.read_csv(dataset_train.path)\n",
    "    test_data = pd.read_csv(dataset_test.path)\n",
    "    \n",
    "    # Instantiate the model class\n",
    "    busyness_model = BusynessEstimation(\n",
    "                                        test_data.copy()\n",
    "                                        )\n",
    "                                        \n",
    "    # Create X_train and y_train\n",
    "    X_train = train_data.drop(Data.target, axis=1)\n",
    "    y_train = train_data[Data.target]\n",
    "\n",
    "    # Fit the model (training pipeline consists of feature engineering, feature selection and training an xgboost model)\n",
    "    busyness_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the best hyperparameters as an artifact\n",
    "    with open(best_params.path, \"w\") as f:\n",
    "        f.write(str(busyness_model.best_params))\n",
    "    \"\"\"    \n",
    "    shap.summary_plot(busyness_model.shap_values, busyness_model.X_test_transformed, max_display=20) # plot shap summary plot\n",
    "    shap_plot_dataurl = get_image_data() # get image data to render the image in the html file\n",
    "    html_content = f'<html><head></head><body><h1>Shap Summary Plot</h1>\\n<img src={shap_plot_dataurl} width=\"97%\"></body></html>' \n",
    "    # Save shap summary plot as an html artifact\n",
    "    with open(shap_summary_plot.path, \"w\") as f: \n",
    "        f.write(html_content)\n",
    "    \"\"\"\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\" \n",
    "    # Save the model as an artifact\n",
    "    with open(model.path, 'wb') as f: \n",
    "        pickle.dump({\n",
    "            \"pipeline\": busyness_model.model_pipeline,\n",
    "            \"target\": busyness_model.target,\n",
    "            \"scores_dict\": busyness_model.scores}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95f191-eca3-4fb0-90f1-6525c525b679",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5030b66-2e40-4215-b41f-3ffe598f2df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def evaluate_busyness(\n",
    "    busyness_model: Input[Model],\n",
    "    metrics_baseline: Output[Metrics],\n",
    "    metrics_train: Output[Metrics],\n",
    "    metrics_test: Output[Metrics]):\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    file_name = busyness_model.path\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model_data = pickle.load(file)\n",
    "    \n",
    "    scores = model_data[\"scores_dict\"] \n",
    "\n",
    "    def log_metrics(scores, metric):\n",
    "        for metric_name, val in scores.items():\n",
    "            metric.log_metric(metric_name, float(val))\n",
    "            \n",
    "    log_metrics(scores[\"baseline_scores\"], metrics_baseline)        \n",
    "    log_metrics(scores[\"train_scores\"], metrics_train)\n",
    "    log_metrics(scores[\"test_scores\"], metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461f86f-bbd1-47ed-a6fa-bff92213239a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a26372-9e07-4edc-9fc0-2a7188aab032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    install_kfp_package=False,\n",
    ")\n",
    "def deploy_busyness(\n",
    "        serving_container_image_uri: str,\n",
    "        display_name: str,\n",
    "        model_endpoint: str,\n",
    "        gcp_project: str,\n",
    "        gcp_region: str,\n",
    "        model: Input[Model],\n",
    "        vertex_model: Output[Model],\n",
    "        vertex_endpoint: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Checks existing Vertex AI Enpoint or creates Endpoint if it is not exist.\n",
    "    def create_endpoint ():\n",
    "        endpoints = vertex_ai.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(model_endpoint),\n",
    "        order_by='create_time desc',\n",
    "        project=gcp_project,\n",
    "        location=gcp_region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0] # most recently created\n",
    "        else:\n",
    "            endpoint = vertex_ai.Endpoint.create(\n",
    "                display_name=model_endpoint,\n",
    "                project=gcp_project,\n",
    "                location=gcp_region\n",
    "        )\n",
    "        return endpoint\n",
    "\n",
    "    endpoint = create_endpoint()\n",
    "    \n",
    "    # Uploads trained model to Vertex AI Model Registry or creates new model version into existing uploaded one.\n",
    "    def upload_model ():\n",
    "        listed_model = vertex_ai.Model.list(\n",
    "        filter='display_name=\"{}\"'.format(display_name),\n",
    "        project=gcp_project,\n",
    "        location=gcp_region,\n",
    "        )\n",
    "        if len(listed_model) > 0:\n",
    "            model_version = listed_model[0] # most recently created\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    parent_model=model_version.resource_name,\n",
    "                    artifact_uri=str(Path(model.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        else:\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    artifact_uri=str(Path(model.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        return model_upload\n",
    "    \n",
    "    uploaded_model = upload_model()\n",
    "    \n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = uploaded_model.resource_name\n",
    "\n",
    "    # Deploys trained model to Vertex AI Endpoint\n",
    "    model_deploy = uploaded_model.deploy(\n",
    "        machine_type='n1-standard-1',\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac68c79-48b6-4e0d-a992-a10db5763e6c",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3559983-00bd-4b73-9085-af190b67950b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipelinebusyness-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365f534-c885-431e-9ac5-3f04c52bbd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-regionsbusyness\"   \n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    data_filepath: str = \"data/data.csv\",\n",
    "    data_bucket : str = BUCKET_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION    \n",
    "):\n",
    "\n",
    "    data_op = get_std_data(gcs_bucket = data_bucket, gcs_path = data_filepath )\n",
    "    data_preprocess_op = preprocess_std_data(train_df = data_op.outputs[\"dataset_train\"])\n",
    "    train_test_split_op = train_test_split(dataset_in = data_preprocess_op.outputs[\"dataset_train_preprocessed\"])\n",
    "    train_model_op = train_busyness(dataset_train = train_test_split_op.outputs[\"dataset_train\"], dataset_test = train_test_split_op.outputs[\"dataset_test\"])\n",
    "    model_evaluation_op = evaluate_busyness(busyness_model = train_model_op.outputs[\"model\"])\n",
    "    \n",
    "    deploy_model_op = deploy_busyness(\n",
    "        model = train_model_op.outputs['model'],\n",
    "        gcp_project = project,\n",
    "        gcp_region = region, \n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        display_name = \"rb\",\n",
    "        model_endpoint = \"rb_endpoint\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c0754",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d7caaf3-c2cd-4a00-a29d-583f0e8832d0",
   "metadata": {},
   "source": [
    "## Compile and Run the pipelie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d586c-a063-4413-a898-87fc4cdd5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE (to create the job spec file)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_rb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1f920-c0a0-40a7-9827-58d15a70a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A RUN USING THE JOB SPEC FILE GENERATED \n",
    "\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"rb-pipeline\",\n",
    "    template_path=\"ml_rb.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8dd691-60b6-464e-ada5-095d3d37f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE PIPELINE\n",
    "\n",
    "start_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39214801-6a5b-4c71-b637-e86a76becc21",
   "metadata": {},
   "source": [
    "## Make Predictions Using Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387d01f-9e27-469b-ac16-2b0d06799b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "endpoint_name = <ENDPOINT_URI>\n",
    "endpoint = vertex_ai.Endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9c51e-8c4d-40d4-8e4b-292a0704d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4019b4c-8521-4d2a-889d-3e216ae5bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = test_df.to_json(orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54de62-aaa0-41bd-a149-4cbb53dae9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=request.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773799b-b90e-4b2d-9a22-8be2b675f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(get_std_data, package_path='get_data.yaml')\n",
    "compiler.Compiler().compile(preprocess_std_data, package_path='preprocessing.yaml')\n",
    "compiler.Compiler().compile(train_test_split, package_path='train_test_split.yaml')\n",
    "compiler.Compiler().compile(train_busyness, package_path='model_training.yaml')\n",
    "compiler.Compiler().compile(evaluate_busyness, package_path='model_evaluation.yaml')\n",
    "compiler.Compiler().compile(deploy_busyness, package_path='model_deployment.yaml')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}